# LinkedIn Scraper & Resume Parser Integration Guide

## üéØ Problem Solved

The knowledge base was not containing information scraped from LinkedIn and resume parsing because:

1. **Hardcoded file paths**: The interview system used specific hardcoded file names instead of auto-detecting the latest data
2. **Under-extraction**: The parsers only extracted minimal data (just names) instead of comprehensive information
3. **No integration**: The existing `linkdin_scrapper.py` and `resume_parser` tools weren't properly connected to the knowledge base

## ‚úÖ Solutions Implemented

### 1. Enhanced Knowledge Base Parsers

**File**: `Brain/lda/knowledge_db.py`

**New LinkedIn Parser** now extracts:
- ‚úÖ Personal info (name, professional summary)
- ‚úÖ Work experiences (5+ positions with companies, roles, durations, locations)
- ‚úÖ Key achievements (15+ accomplishments from job descriptions)
- ‚úÖ Education (degrees, institutions, GPAs)
- ‚úÖ Skills (20+ technical skills)
- ‚úÖ Projects (9+ projects with timeframes and technologies)
- ‚úÖ Certifications (professional certifications)
- ‚úÖ Honors and awards

**New Resume Parser** now extracts:
- ‚úÖ Personal info (name from header)
- ‚úÖ Education (degrees, institutions, GPAs)
- ‚úÖ Work experience (internships, positions)
- ‚úÖ Technical skills (60+ skills)
- ‚úÖ Projects (significant projects)
- ‚úÖ Achievements (academic merit, competition rankings)
- ‚úÖ Courses (university courses)

### 2. Auto-Detection of Latest Data

**New Function**: `auto_load_latest_candidate_data()`

```python
# Before (hardcoded):
linkedin_file = "logs/person_Vatsal_Jain_20250923_070456.json"
resume_file = "logs/resume_1757659381503_20250923T013457.json"

# After (auto-detection):
loaded_files = knowledge_db.auto_load_latest_candidate_data(project_root)
```

- Automatically finds the latest `person_*.json` (LinkedIn) files
- Automatically finds the latest `resume_*.json` files  
- Can filter by candidate name
- Provides fallback to hardcoded files for backwards compatibility

### 3. Integration with Existing Scrapers

**New File**: `Brain/lda/scraper_integration.py`

**LinkedIn Scraper Integration**:
- Uses existing `linkdin_scrapper.py`
- Accepts any LinkedIn URL (not hardcoded)
- Manages Selenium automation
- Saves to standard `logs/person_*.json` format

**Resume Parser Integration**:
- Uses existing `resume_parser/parse_resume.ts`
- Processes any PDF resume
- Runs TypeScript parser via npm/tsx
- Saves to standard `logs/resume_*.json` format

### 4. Unified Data Collection Workflow

**New File**: `collect_candidate_data.py`

Complete workflow script that:
- Scrapes LinkedIn profiles using existing scraper
- Parses resume PDFs using existing parser
- Auto-loads latest existing data files
- Integrates everything into the knowledge base
- Provides comprehensive reporting

## üöÄ Usage Examples

### Auto-Load Existing Data
```bash
# Load the latest candidate data files automatically
python collect_candidate_data.py --auto-load

# Load data for specific candidate
python collect_candidate_data.py --auto-load --candidate-name "Vatsal Jain"
```

### Scrape New LinkedIn Data
```bash
# Set environment variables
export LINKEDIN_EMAIL="your_email@example.com"
export LINKEDIN_PASSW="your_password"

# Scrape LinkedIn profile
python collect_candidate_data.py --linkedin-url "https://linkedin.com/in/username"
```

### Parse New Resume
```bash
# Parse resume PDF
python collect_candidate_data.py --resume-pdf "path/to/resume.pdf"
```

### Complete Data Collection
```bash
# Scrape LinkedIn + parse resume in one go
python collect_candidate_data.py \
  --linkedin-url "https://linkedin.com/in/username" \
  --resume-pdf "path/to/resume.pdf"
```

### Using Individual Scrapers

**LinkedIn Scraper** (`linkdin_scrapper.py`):
```bash
export LINKEDIN_EMAIL="email@example.com"
export LINKEDIN_PASSW="password"
python linkdin_scrapper.py  # Uses hardcoded URL

# To modify for different URL, edit line 82:
# person = Person("https://www.linkedin.com/in/your-profile/", driver=driver)
```

**Resume Parser** (`resume_parser/parse_resume.ts`):
```bash
cd resume_parser
npx tsx parse_resume.ts path/to/resume.pdf
```

## üìä Results

The enhanced system now extracts **143+ entries** from candidate data:

**Before**: 1-2 entries (just names)
**After**: 143 entries across multiple categories

| Category | LinkedIn | Resume | Total |
|----------|----------|---------|-------|
| Personal | 2 | 1 | 3 |
| Experience | 5 | 3 | 8 |
| Education | 6 | 11 | 17 |
| Skills | 20 | 60 | 80 |
| Projects | 9 | 8 | 17 |
| Achievements | 15 | 0 | 15 |
| Certifications | 3 | 0 | 3 |

## üîß Technical Integration

### Modified Files

1. **`Brain/lda/knowledge_db.py`**
   - Enhanced `_parse_linkedin_data()` and `_parse_resume_data()`
   - Added `auto_load_latest_candidate_data()`
   - Comprehensive data extraction from existing JSON structures

2. **`Brain/lda/main.py`**
   - Replaced hardcoded file paths with auto-detection
   - Added fallback for backwards compatibility

3. **New Files**:
   - `Brain/lda/scraper_integration.py` - Integration layer
   - `collect_candidate_data.py` - Unified workflow script

### Data Flow

```
LinkedIn Profile ‚Üí linkdin_scrapper.py ‚Üí logs/person_*.json
Resume PDF ‚Üí resume_parser/parse_resume.ts ‚Üí logs/resume_*.json
Latest Files ‚Üí auto_load_latest_candidate_data() ‚Üí Enhanced Knowledge Base
Knowledge Base ‚Üí Interview System ‚Üí Comprehensive Candidate Information
```

## üéØ Interview System Impact

The interview system now has access to comprehensive candidate information including:

- **Work Experience**: Atlassian internship, IIT Delhi positions, UNSW research
- **Technical Skills**: Python, React.js, Machine Learning, AI, etc.
- **Education**: IIT Delhi B.Tech with 8.81 GPA
- **Projects**: AI Havannah Game, Stock Market Simulation, Computer Vision
- **Achievements**: Merit awards, competition rankings, certifications
- **Certifications**: Machine Learning courses from Coursera/DeepLearning.AI

This enables:
- ‚úÖ **Accurate lie detection** against comprehensive verified facts
- ‚úÖ **Informed questioning** based on candidate's actual background
- ‚úÖ **Contextual follow-ups** about specific experiences and projects
- ‚úÖ **Skill validation** against claimed technical expertise

## üèÉ‚Äç‚ôÇÔ∏è Quick Start

1. **Use existing data**:
   ```bash
   python collect_candidate_data.py --auto-load
   ```

2. **Run interview with enhanced knowledge base**:
   ```bash
   cd Brain/lda
   python main.py
   ```

The system will automatically load the latest candidate data and provide comprehensive context for the interview!
